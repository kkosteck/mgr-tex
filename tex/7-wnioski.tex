\newpage % Rozdziały zaczynamy od nowej strony.
\section{Wnioski}
Najlepszym rozwiązaniem spośród wszelkich kombinacji modułów kodujących oraz dekodujących okazało się najbardziej zaawansowane połączenie, czyli transformer wizyjny wraz z klasycznym transformerem. Niestety takie rozwiązanie i tak odstaje od kompleksowych architektur stworzonych z myślą o generowaniu podpisów do obrazów, takich jak GIT i OFA. O wiele lepsza skuteczność tychże rozwiązań jednocześnie wiąże się z o wiele większymi wymaganiami sprzętowymi, które graniczą z możliwością ich wykorzystania. Szczególnie dotkliwe jest wymaganie alokacji dużej ilości pamięci, co skutkuje brakiem możliwości wykorzystania mniej zaawansowanych wersji jednostek GPU. Jest to główne ograniczenie, ponieważ w przypadku sieci, które były w stanie zostać uruchomione za pomocą mniejszej karty graficznej zostały przetworzone w podobnym czasie do tego przy wykorzystaniu znacznie bardziej zaawansowanej jednostki GPU. W przypadku dedykowanych architektur zarówno czas uczenia, jak i generowania podpisów jest znacząco większy niż w przypadku wykorzystywania mniej złożonych sieci. Pokazuje to, iż rozwój sztucznej inteligencji kieruje się w stronę coraz bardziej rozbudowanych i obciążających rozwiązań. Otrzymanie wyników bez wykorzystywania wstępnie wyuczonych modeli staje się coraz trudniejsze ze względu na potrzebę posiadania wielkiego zbioru danych oraz zasobów obliczeniowych. W przypadku architektury GIT czy OFA warto również pamiętać, że wstępnie wyuczone modele nie rozwiązują problemu późniejszych wymagań obliczeniowych potrzebnych w trakcie docelowego generowania podpisów. Jednoznacznie można stwierdzić, iż podstawowa sieć rekurencyjna nie jest w stanie sprostać zadaniu generowania podpisów do obrazów, a jej bardziej zaawansowani następcy jak LSTM czy GRU są o wiele lepszym wyborem. W przypadku sieci splotowych wybór najlepszego rozwiązania nie jest tak oczywisty. Wraz z lepszą skutecznością rosną wymagania sprzętowe. Tak duże wymagania obliczeniowe, jakie występują w procesie uczenia maszynowego, wywierają znaczący wpływ na czas potrzebny na wyuczenie modelu, co może stanowić jedno z kluczowych ograniczeń w dalszym rozwoju sztucznej inteligencji. Proces szkolenia modeli maszynowych wymaga ogromnych nakładów czasu zarówno na przetwarzanie, jak i adaptację do coraz bardziej rosnących zbiorów danych. Model uczenia maszynowego musi być zdolny do efektywnego przetwarzania ogromnych ilości informacji, a jednocześnie szybko dostosowywać się do zmieniających się warunków. Owa potrzeba adaptacji jest szczególnie ważna w kontekście dynamicznie rozwijającego się świata, gdzie ilość generowanych danych z dnia na dzień znacząco rośnie. Dzięki temu modele mogą lepiej reprezentować rzeczywistość i dostosowywać się do zmieniających się wzorców. Otrzymane wyniki w kwestii czasu potrzebnego na przetwarzanie architektur sieci neuronowych, jak również wymagania sprzętowe pokazują, iż może stanowić to duże ograniczenie w dalszym rozwoju tychże rozwiązań.
\subsection{Perspektywy rozwoju}
% MAYBE: pomysł na lepsze połączenie, lepsze wykorzystanie dużej ilości danych sieci splotowej
Ze względu na ograniczenia czasowe nie było możliwe zweryfikowanie, jak duży wpływ ma równoczesne trenowanie modułu kodującego wraz z modułem dekodującym na skuteczność otrzymanych rozwiązań. Można przypuszczać, iż skuteczność w pewnym stopniu by wzrosła, ale są to jedynie domysły, które warte są weryfikacji. Być może wartym zweryfikowania byłyby również inne architektury modułów kodujących i dekodujących stworzone z myślą o większej wydajności.
